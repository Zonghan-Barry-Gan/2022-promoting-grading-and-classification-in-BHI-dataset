{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Detecting Brain Tumors by extracting features using deep neural networks and then using Classical ML Models for prediction\n\n<h3>author : Abhranta Panigrahi</h3>\n<h3>date :  6th March, 2021</h3>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"markdown","source":"<h2> DataSet Description </h2>\n\n<p> The image data that was used for this problem is <a href = \"https://www.kaggle.com/abhranta/brain-tumor-detection-mri\">Brain MRI scans for tumor detection</a>. It consists of MRI scans of the human brain. It has three sub-directories:\n        <ul>\n            <li><b>no</b> :  These are the MRI scans of the brain that have no tumors. These are labelled as 0.</li>\n            <li><b>yes</b> : These are the MRI scans of the brain that have a tumor, These are labelles as 1. </li>\n            <li><b>pred</b> : These images are the unlabelled images. These are meant to be used as the test set. </li>\n    ","metadata":{}},{"cell_type":"markdown","source":"<h2> Brain Tumor</h2>\n<p>>A brain tumor is a collection, or mass, of abnormal cells in your brain. Your skull, which encloses your brain, is very rigid. Any growth inside such a restricted space can cause problems. Brain tumors can be cancerous (malignant) or noncancerous (benign). When benign or malignant tumors grow, they can cause the pressure inside your skull to increase. This can cause brain damage, and it can be life-threatening.</p>\n<img src = \"https://img.medscapestatic.com/pi/features/slideshow-slide/pediatric-brain-tumors-6009019/fig1.jpg?resize=580:*\">","metadata":{}},{"cell_type":"markdown","source":"<h2> MRI </h2>\n<p>Magnetic resonance imaging (MRI) is a medical imaging technique used in radiology to form pictures of the anatomy and the physiological processes of the body. MRI scanners use strong magnetic fields, magnetic field gradients, and radio waves to generate images of the organs in the body. MRI does not involve X-rays or the use of ionizing radiation, which distinguishes it from CT and PET scans. MRI is a medical application of nuclear magnetic resonance (NMR) which can also be used for imaging in other NMR applications, such as NMR spectroscopy.</p>\n<img src = \"https://i.pinimg.com/originals/49/6d/95/496d952e43d6a3b9aa87ef63a73e11fe.gif\">","metadata":{}},{"cell_type":"markdown","source":"<h2> In this notebook we will be extracting the features using Neural Networks and using standard ML Classifiers to classify the data based on the extracted features. </h2>\n\n<h3>Neural networks used </h3>\n<ol>\n    <li><b>VGG-16</b></li>\n    <li><b>VGG-19</b></li>\n    <li><b>ResNet50</b></li>\n    <li><b>ResNet101</b></li>\n    <li><b>MobileNetv2</b></li>\n    <li><b>MobileNet</b></li>\n    <li><b>Inceptionv3</b></li>\n    <li><b>InceptionResnetv2</b></li>\n    <li><b>DenseNet169</b></li>\n    <li><b>DenseNet121</b></li>\n    <li><b>XceptionNet</b></li>\n</ol>\n<h3>Classifiers used</h3>\n<ol>\n    <li><b>ANN</b></li>\n    <li><b>SVM => SVC</b></li>\n    <li><b>Random Forest Classifier</b></li>\n    <li><b>AdaBoost Classifier</b></li>\n    <li><b>XGBoost Classifier</b></li>\n    <li><b> KNN Classifier</b></li>\n   ","metadata":{}},{"cell_type":"markdown","source":"<h3>1. Import all the necessary Libraries</h3>","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output\n!pip install imutils","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:52:26.999575Z","iopub.execute_input":"2022-03-29T08:52:27.000048Z","iopub.status.idle":"2022-03-29T08:52:37.738250Z","shell.execute_reply.started":"2022-03-29T08:52:26.999937Z","shell.execute_reply":"2022-03-29T08:52:37.737189Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nimport scipy\nimport  os\nimport tensorflow as tf\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.preprocessing.image import *\nfrom tensorflow.keras.utils import *\nimport shutil\nimport itertools\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\n# import pydot\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nimport tensorflow.keras.backend as K\n\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom tqdm import tqdm, tqdm_notebook\nfrom colorama import Fore\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nfrom skimage.io import *\n%config Completer.use_jedi = False\nimport time\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n\nprint(\"All modules have been imported\")","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:52:37.741268Z","iopub.execute_input":"2022-03-29T08:52:37.741615Z","iopub.status.idle":"2022-03-29T08:52:45.338759Z","shell.execute_reply.started":"2022-03-29T08:52:37.741578Z","shell.execute_reply":"2022-03-29T08:52:45.337248Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Right now all images are in one folder with `yes` and `no` subfolders. I will split the data into `train`, `val` and `test` folders which makes its easier to work for me. The new folder heirarchy will look as follows:","metadata":{}},{"cell_type":"code","source":"!apt-get install tree\nclear_output()\n# create new folders\n!mkdir TRAIN TEST VAL TRAIN/YES TRAIN/NO TEST/YES TEST/NO VAL/YES VAL/NO\n!tree -d","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:52:45.340473Z","iopub.execute_input":"2022-03-29T08:52:45.340861Z","iopub.status.idle":"2022-03-29T08:52:50.868789Z","shell.execute_reply.started":"2022-03-29T08:52:45.340828Z","shell.execute_reply":"2022-03-29T08:52:50.867734Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"IMG_PATH = \"../input/brain-tumor-detection-mri/Brain_Tumor_Detection\"\n\n# split the data by train/val/test\nignored = {\"pred\"}\n# split the data by train/val/test\nfor CLASS in os.listdir(IMG_PATH):\n    if CLASS not in ignored:\n        if not CLASS.startswith('.'):\n            IMG_NUM = len(os.listdir(IMG_PATH +\"/\"+ CLASS))\n            for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH +\"/\"+ CLASS)):\n                img = IMG_PATH+ '/' +  CLASS + '/' + FILE_NAME\n                if n < 300:\n                    shutil.copy(img, 'TEST/' + CLASS.upper() + '/' + FILE_NAME)\n                elif n < 0.8*IMG_NUM:\n                    shutil.copy(img, 'TRAIN/'+ CLASS.upper() + '/' + FILE_NAME)\n                else:\n                    shutil.copy(img, 'VAL/'+ CLASS.upper() + '/' + FILE_NAME)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:52:50.870576Z","iopub.execute_input":"2022-03-29T08:52:50.870907Z","iopub.status.idle":"2022-03-29T08:53:00.326061Z","shell.execute_reply.started":"2022-03-29T08:52:50.870873Z","shell.execute_reply":"2022-03-29T08:53:00.324924Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<h3>Data Importing and Preprocessing</h3>","metadata":{}},{"cell_type":"code","source":"def load_data(dir_path, img_size=(100,100)):\n    \"\"\"\n    Load resized images as np.arrays to workspace\n    \"\"\"\n    X = []\n    y = []\n    i = 0\n    labels = dict()\n    for path in tqdm(sorted(os.listdir(dir_path))):\n        if not path.startswith('.'):\n            labels[i] = path\n            for file in os.listdir(dir_path + path):\n                if not file.startswith('.'):\n                    img = cv2.imread(dir_path + path + '/' + file)\n                    X.append(img)\n                    y.append(i)\n            i += 1\n    X = np.array(X)\n    y = np.array(y)\n    print(f'{len(X)} images loaded from {dir_path} directory.')\n    return X, y, labels\n\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    cm = np.round(cm,2)\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:00.331138Z","iopub.execute_input":"2022-03-29T08:53:00.331442Z","iopub.status.idle":"2022-03-29T08:53:00.346940Z","shell.execute_reply.started":"2022-03-29T08:53:00.331413Z","shell.execute_reply":"2022-03-29T08:53:00.345724Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"TRAIN_DIR = 'TRAIN/'\nTEST_DIR = 'TEST/'\nVAL_DIR = 'VAL/'\nIMG_SIZE = (224,224)\n\n# use predefined function to load the image data into workspace\nX_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\nX_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\nX_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:00.350588Z","iopub.execute_input":"2022-03-29T08:53:00.350926Z","iopub.status.idle":"2022-03-29T08:53:07.290890Z","shell.execute_reply.started":"2022-03-29T08:53:00.350896Z","shell.execute_reply":"2022-03-29T08:53:07.289635Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"y = dict()\ny[0] = []\ny[1] = []\nfor set_name in (y_train, y_val, y_test):\n    y[0].append(np.sum(set_name == 0))\n    y[1].append(np.sum(set_name == 1))\n\ntrace0 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[0],\n    name='No',\n    marker=dict(color='#33cc33'),\n    opacity=0.7\n)\ntrace1 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[1],\n    name='Yes',\n    marker=dict(color='#ff3300'),\n    opacity=0.7\n)\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title='Count of classes in each set',\n    xaxis={'title': 'Set'},\n    yaxis={'title': 'Count'}\n)\nfig = go.Figure(data, layout)\niplot(fig)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:07.292749Z","iopub.execute_input":"2022-03-29T08:53:07.293073Z","iopub.status.idle":"2022-03-29T08:53:08.531177Z","shell.execute_reply.started":"2022-03-29T08:53:07.293045Z","shell.execute_reply":"2022-03-29T08:53:08.529839Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def plot_samples(X, y, labels_dict, n=50):\n    \"\"\"\n    Creates a gridplot for desired number of images (n) from the specified set\n    \"\"\"\n    for index in range(len(labels_dict)):\n        imgs = X[np.argwhere(y == index)][:n]\n        j = 10\n        i = int(n/j)\n\n        plt.figure(figsize=(15,6))\n        c = 1\n        for img in imgs:\n            plt.subplot(i,j,c)\n            plt.imshow(img[0])\n\n            plt.xticks([])\n            plt.yticks([])\n            c += 1\n        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:08.532958Z","iopub.execute_input":"2022-03-29T08:53:08.533352Z","iopub.status.idle":"2022-03-29T08:53:08.542419Z","shell.execute_reply.started":"2022-03-29T08:53:08.533312Z","shell.execute_reply":"2022-03-29T08:53:08.541180Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"plot_samples(X_train, y_train, labels, 30)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:08.543548Z","iopub.execute_input":"2022-03-29T08:53:08.543938Z","iopub.status.idle":"2022-03-29T08:53:12.194224Z","shell.execute_reply.started":"2022-03-29T08:53:08.543898Z","shell.execute_reply":"2022-03-29T08:53:12.193175Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"As you can see, images have different width and height and diffent size of \"black corners\". Since the image size for VGG-16 imput layer is (224,224) some wide images may look weird after resizing. Histogram of ratio distributions (ratio = width/height):","metadata":{}},{"cell_type":"code","source":"RATIO_LIST = []\nfor set in (X_train, X_test, X_val):\n    for img in set:\n        RATIO_LIST.append(img.shape[1]/img.shape[0])\n        \nplt.hist(RATIO_LIST)\nplt.title('Distribution of Image Ratios')\nplt.xlabel('Ratio Value')\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:12.195718Z","iopub.execute_input":"2022-03-29T08:53:12.196068Z","iopub.status.idle":"2022-03-29T08:53:12.391394Z","shell.execute_reply.started":"2022-03-29T08:53:12.196035Z","shell.execute_reply":"2022-03-29T08:53:12.390652Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"<p> First I will be cropping the  images out so that they noly contain the brain and no excess data.</p>\n<p> This was done by finding extreme points in image contours<p>","metadata":{}},{"cell_type":"code","source":"def crop_imgs(set_name, add_pixels_value=0):\n    \"\"\"\n    Finds the extreme points on the image and crops the rectangular out of them\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # threshold the image, then perform a series of erosions +\n        # dilations to remove any small regions of noise\n        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n        thresh = cv2.erode(thresh, None, iterations=2)\n        thresh = cv2.dilate(thresh, None, iterations=2)\n\n        # find contours in thresholded image, then grab the largest one\n        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts = imutils.grab_contours(cnts)\n        c = max(cnts, key=cv2.contourArea)\n\n        # find the extreme points\n        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n        extRight = tuple(c[c[:, :, 0].argmax()][0])\n        extTop = tuple(c[c[:, :, 1].argmin()][0])\n        extBot = tuple(c[c[:, :, 1].argmax()][0])\n\n        ADD_PIXELS = add_pixels_value\n        new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n        set_new.append(new_img)\n\n    return np.array(set_new)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:12.392603Z","iopub.execute_input":"2022-03-29T08:53:12.393044Z","iopub.status.idle":"2022-03-29T08:53:12.404206Z","shell.execute_reply.started":"2022-03-29T08:53:12.393007Z","shell.execute_reply":"2022-03-29T08:53:12.402898Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import imutils\nimg = cv2.imread('./VAL/NO/no852.jpg')\nimg = cv2.resize(\n            img,\n            dsize=IMG_SIZE,\n            interpolation=cv2.INTER_CUBIC\n        )\ngray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\ngray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n# threshold the image, then perform a series of erosions +\n# dilations to remove any small regions of noise\nthresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\nthresh = cv2.erode(thresh, None, iterations=2)\nthresh = cv2.dilate(thresh, None, iterations=2)\n\n# find contours in thresholded image, then grab the largest one\ncnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\nc = max(cnts, key=cv2.contourArea)\n\n# find the extreme points\nextLeft = tuple(c[c[:, :, 0].argmin()][0])\nextRight = tuple(c[c[:, :, 0].argmax()][0])\nextTop = tuple(c[c[:, :, 1].argmin()][0])\nextBot = tuple(c[c[:, :, 1].argmax()][0])\n\n# add contour on the image\nimg_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n\n# add extreme points\nimg_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\nimg_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n\n# crop\nADD_PIXELS = 0\nnew_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:12.406038Z","iopub.execute_input":"2022-03-29T08:53:12.406492Z","iopub.status.idle":"2022-03-29T08:53:12.480089Z","shell.execute_reply.started":"2022-03-29T08:53:12.406446Z","shell.execute_reply":"2022-03-29T08:53:12.479234Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.subplot(141)\nplt.imshow(img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 1. Get the original image')\nplt.subplot(142)\nplt.imshow(img_cnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 2. Find the biggest contour')\nplt.subplot(143)\nplt.imshow(img_pnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 3. Find the extreme points')\nplt.subplot(144)\nplt.imshow(new_img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 4. Crop the image')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:12.481330Z","iopub.execute_input":"2022-03-29T08:53:12.481958Z","iopub.status.idle":"2022-03-29T08:53:12.738105Z","shell.execute_reply.started":"2022-03-29T08:53:12.481916Z","shell.execute_reply":"2022-03-29T08:53:12.736913Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# apply this for each set\nX_train_crop = crop_imgs(set_name=X_train)\nX_val_crop = crop_imgs(set_name=X_val)\nX_test_crop = crop_imgs(set_name=X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:12.739488Z","iopub.execute_input":"2022-03-29T08:53:12.739976Z","iopub.status.idle":"2022-03-29T08:53:16.102763Z","shell.execute_reply.started":"2022-03-29T08:53:12.739935Z","shell.execute_reply":"2022-03-29T08:53:16.101914Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"plot_samples(X_train_crop, y_train, labels, 30)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:16.104168Z","iopub.execute_input":"2022-03-29T08:53:16.104672Z","iopub.status.idle":"2022-03-29T08:53:19.450048Z","shell.execute_reply.started":"2022-03-29T08:53:16.104640Z","shell.execute_reply":"2022-03-29T08:53:19.449005Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"RATIO_LIST = []\nfor set in (X_train_crop, X_test_crop, X_val_crop):\n    for img in set:\n        RATIO_LIST.append(img.shape[1]/img.shape[0])\n        \nplt.hist(RATIO_LIST)\nplt.title('Distribution of Image Ratios')\nplt.xlabel('Ratio Value')\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:19.451512Z","iopub.execute_input":"2022-03-29T08:53:19.451826Z","iopub.status.idle":"2022-03-29T08:53:19.650035Z","shell.execute_reply.started":"2022-03-29T08:53:19.451787Z","shell.execute_reply":"2022-03-29T08:53:19.648489Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def save_new_images(x_set, y_set, folder_name):\n    i = 0\n    for (img, imclass) in zip(x_set, y_set):\n        if imclass == 0:\n            cv2.imwrite(folder_name+'NO/'+str(i)+'.jpg', img)\n        else:\n            cv2.imwrite(folder_name+'YES/'+str(i)+'.jpg', img)\n        i += 1","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:19.652133Z","iopub.execute_input":"2022-03-29T08:53:19.652579Z","iopub.status.idle":"2022-03-29T08:53:19.659843Z","shell.execute_reply.started":"2022-03-29T08:53:19.652534Z","shell.execute_reply":"2022-03-29T08:53:19.658549Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# saving new images to the folder\n!mkdir TRAIN_CROP TEST_CROP VAL_CROP TRAIN_CROP/YES TRAIN_CROP/NO TEST_CROP/YES TEST_CROP/NO VAL_CROP/YES VAL_CROP/NO\n\nsave_new_images(X_train_crop, y_train, folder_name='TRAIN_CROP/')\nsave_new_images(X_val_crop, y_val, folder_name='VAL_CROP/')\nsave_new_images(X_test_crop, y_test, folder_name='TEST_CROP/')","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:19.661232Z","iopub.execute_input":"2022-03-29T08:53:19.661588Z","iopub.status.idle":"2022-03-29T08:53:27.071792Z","shell.execute_reply.started":"2022-03-29T08:53:19.661557Z","shell.execute_reply":"2022-03-29T08:53:27.070607Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"<h2>Resizing the Images</h2>\n\n<p> Since all the neural entwork models take input of size (224x224x3) hence all the images were interpolated using Inter Cubic interpolation and open cv </p>","metadata":{}},{"cell_type":"code","source":"def preprocess_imgs(set_name, img_size):\n    set_new = []\n    for img in set_name:\n        img = cv2.resize(\n            img,\n            dsize=img_size,\n            interpolation=cv2.INTER_CUBIC\n        )\n        set_new.append(preprocess_input(img))\n    return np.array(set_new)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:27.073700Z","iopub.execute_input":"2022-03-29T08:53:27.074059Z","iopub.status.idle":"2022-03-29T08:53:27.080489Z","shell.execute_reply.started":"2022-03-29T08:53:27.074011Z","shell.execute_reply":"2022-03-29T08:53:27.079390Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"X_train_prep = preprocess_imgs(set_name=X_train_crop, img_size=IMG_SIZE)\nX_test_prep = preprocess_imgs(set_name=X_test_crop, img_size=IMG_SIZE)\nX_val_prep = preprocess_imgs(set_name=X_val_crop, img_size=IMG_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:27.082247Z","iopub.execute_input":"2022-03-29T08:53:27.083005Z","iopub.status.idle":"2022-03-29T08:53:32.615849Z","shell.execute_reply.started":"2022-03-29T08:53:27.082957Z","shell.execute_reply":"2022-03-29T08:53:32.614651Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"plot_samples(X_train_prep, y_train, labels, 30)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:32.617680Z","iopub.execute_input":"2022-03-29T08:53:32.618115Z","iopub.status.idle":"2022-03-29T08:53:36.242157Z","shell.execute_reply.started":"2022-03-29T08:53:32.618073Z","shell.execute_reply":"2022-03-29T08:53:36.241089Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"RATIO_LIST = []\nfor set in (X_train_prep, X_test_prep, X_val_prep):\n    for img in set:\n        RATIO_LIST.append(img.shape[1]/img.shape[0])\n        \nplt.hist(RATIO_LIST)\nplt.title('Distribution of Image Ratios')\nplt.xlabel('Ratio Value')\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:36.243642Z","iopub.execute_input":"2022-03-29T08:53:36.243946Z","iopub.status.idle":"2022-03-29T08:53:36.428688Z","shell.execute_reply.started":"2022-03-29T08:53:36.243919Z","shell.execute_reply":"2022-03-29T08:53:36.427515Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"<h2>Image Augmentation</h2>\n<p> As the number of training samples is not that great, hence all the images went through a few augmentations to iirease the number of samples. </p>","metadata":{}},{"cell_type":"code","source":"# set the paramters we want to change randomly\ndemo_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    rescale=1./255,\n    shear_range=0.05,\n    brightness_range=[0.1, 1.5],\n    horizontal_flip=True,\n    vertical_flip=True\n)\n\nos.mkdir('preview')\nx = X_train_crop[0]  \nx = x.reshape((1,) + x.shape) \n\ni = 0\nfor batch in demo_datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='aug_img', save_format='jpg'):\n    i += 1\n    if i > 20:\n        break \n        \nplt.imshow(X_train_crop[0])\nplt.xticks([])\nplt.yticks([])\nplt.title('Original Image')\nplt.show()\n\nplt.figure(figsize=(15,6))\ni = 1\nfor img in os.listdir('preview/'):\n    img = cv2.cv2.imread('preview/' + img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.subplot(3,7,i)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    i += 1\n    if i > 3*7:\n        break\nplt.suptitle('Augemented Images')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:36.430324Z","iopub.execute_input":"2022-03-29T08:53:36.430603Z","iopub.status.idle":"2022-03-29T08:53:37.593675Z","shell.execute_reply.started":"2022-03-29T08:53:36.430577Z","shell.execute_reply":"2022-03-29T08:53:37.592663Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"!rm -rf preview/","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:37.595334Z","iopub.execute_input":"2022-03-29T08:53:37.595669Z","iopub.status.idle":"2022-03-29T08:53:38.425767Z","shell.execute_reply.started":"2022-03-29T08:53:37.595637Z","shell.execute_reply":"2022-03-29T08:53:38.423479Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"TRAIN_DIR = 'TRAIN_CROP/'\nVAL_DIR = 'VAL_CROP/'\nRANDOM_SEED = 42\ntrain_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    brightness_range=[0.5, 1.5],\n    horizontal_flip=True,\n    vertical_flip=True,\n    preprocessing_function=preprocess_input\n)\n\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input\n)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=32,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)\n\n\nvalidation_generator = test_datagen.flow_from_directory(\n    VAL_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=16,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:38.430141Z","iopub.execute_input":"2022-03-29T08:53:38.430704Z","iopub.status.idle":"2022-03-29T08:53:38.657420Z","shell.execute_reply.started":"2022-03-29T08:53:38.430645Z","shell.execute_reply":"2022-03-29T08:53:38.656377Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"<h2> Now we willl be creating our ML Classifiers </h2>","metadata":{}},{"cell_type":"markdown","source":"<h2> Building the Classification Pipeline </h2>","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import metrics\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\",\n        \"ANN Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(),\n    SVC(probability = True),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    XGBClassifier(),\n    MLPClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    y_pred_train = [1 if x>0.5 else 0 for x in y_pred_train]\n    y_pred_val = [1 if x>0.5 else 0 for x in y_pred_val]\n    y_pred_test = [1 if x>0.5 else 0 for x in y_pred_test]\n\n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2)\n    test_roc_auc = metrics.roc_auc_score(y_test, y_pred_test ,multi_class='ovo', average='weighted')\n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    confusion_mtx = confusion_matrix(y_train, y_pred_train) \n    cm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    confusion_mtx = confusion_matrix(y_val, y_pred_val) \n    cm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n    \n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    print(\"ROC AUC score : {}\".format(test_roc_auc))\n    confusion_mtx = confusion_matrix(y_test, y_pred_test) \n    cm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n    \n    print('\\t\\tClassification Report:\\n', metrics.classification_report(y_test, y_pred_test))\n    \n    print(\"-\"*80)\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:38.658933Z","iopub.execute_input":"2022-03-29T08:53:38.659636Z","iopub.status.idle":"2022-03-29T08:53:38.700484Z","shell.execute_reply.started":"2022-03-29T08:53:38.659591Z","shell.execute_reply":"2022-03-29T08:53:38.699101Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:38.701913Z","iopub.execute_input":"2022-03-29T08:53:38.702227Z","iopub.status.idle":"2022-03-29T08:53:38.708526Z","shell.execute_reply.started":"2022-03-29T08:53:38.702197Z","shell.execute_reply":"2022-03-29T08:53:38.707280Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"<h2> Training </h2>\n<p> From here on, we will be using various neural networks woith pretrained Image Net weights and extracting a latent representation of our data. This representation will then be used by classical ML models such as K Nearest Neighbours, Support Vector MAchines, etc. to perform the classification task. Various metrics will then be used to evaluate the performance of the models. ","metadata":{}},{"cell_type":"markdown","source":"# ResNet-50","metadata":{"trusted":true}},{"cell_type":"code","source":"base_model= ResNet50(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Dropout(0.5)(x)\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\npredictions = Activation('relu')(x)\n#x = Dropout(0.5)(x)\n\n#predictions = Dense(1, activation='sigmoid')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(X_train_prep)\nval_features=model_feat.predict(X_val_prep)\ntest_features=model_feat.predict(X_test_prep)\n\nprint(train_features.shape)\nprint(val_features.shape)\nprint(test_features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:53:38.710158Z","iopub.execute_input":"2022-03-29T08:53:38.710872Z","iopub.status.idle":"2022-03-29T08:58:42.846009Z","shell.execute_reply.started":"2022-03-29T08:53:38.710837Z","shell.execute_reply":"2022-03-29T08:58:42.844022Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def print_performance_metrics(y_test, y_pred):\n    print('Test_Accuracy:', np.round(metrics.accuracy_score(y_test,y_pred),4))\n    print('Precision:', np.round(metrics.precision_score(y_test,y_pred,average = 'weighted'),4))\n    print('Recall:', np.round(metrics.recall_score(y_test,y_pred,average = 'weighted'),4))\n    print('F1 Score:', np.round(metrics.f1_score(y_test,y_pred,average = 'weighted'),4))\n    print('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test,y_pred)))\n    print('Matthews Corrcoef:', np.round(metrics.matthews_corrcoef(y_test,y_pred)))\n    print('\\t\\tClassification Report:\\n', metrics.classification_report(y_pred,y_test))","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:58:42.848908Z","iopub.execute_input":"2022-03-29T08:58:42.849386Z","iopub.status.idle":"2022-03-29T08:58:42.859521Z","shell.execute_reply.started":"2022-03-29T08:58:42.849339Z","shell.execute_reply":"2022-03-29T08:58:42.858696Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n\n#Create the Model\nclf = KNeighborsClassifier(n_neighbors=1)\n#Train the model with Training Dataset\nclf.fit(train_features,y_train)\n\n#Train accuracy\nypred_train = clf.predict(train_features)\nprint('Train_Accuracy:', np.round(metrics.accuracy_score(y_train,ypred_train),4))\n\n#Test the model with Testset\nypred = clf.predict(test_features)\n\nprint_performance_metrics(y_test, ypred)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:58:42.861070Z","iopub.execute_input":"2022-03-29T08:58:42.861577Z","iopub.status.idle":"2022-03-29T08:58:43.242403Z","shell.execute_reply.started":"2022-03-29T08:58:42.861537Z","shell.execute_reply":"2022-03-29T08:58:43.241021Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:58:43.244361Z","iopub.execute_input":"2022-03-29T08:58:43.244814Z","iopub.status.idle":"2022-03-29T08:58:54.622580Z","shell.execute_reply.started":"2022-03-29T08:58:43.244745Z","shell.execute_reply":"2022-03-29T08:58:54.621737Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# VGG-16","metadata":{}},{"cell_type":"code","source":"base_model= VGG16(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Dropout(0.5)(x)\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\npredictions = Activation('relu')(x)\n#x = Dropout(0.5)(x)\n#predictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(X_train_prep)\nval_features=model_feat.predict(X_val_prep)\ntest_features=model_feat.predict(X_test_prep)\n\nprint(train_features.shape)\nprint(val_features.shape)\nprint(test_features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:58:54.624084Z","iopub.execute_input":"2022-03-29T08:58:54.624382Z","iopub.status.idle":"2022-03-29T09:12:38.070349Z","shell.execute_reply.started":"2022-03-29T08:58:54.624353Z","shell.execute_reply":"2022-03-29T09:12:38.068996Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:12:38.072616Z","iopub.execute_input":"2022-03-29T09:12:38.073126Z","iopub.status.idle":"2022-03-29T09:12:38.079348Z","shell.execute_reply.started":"2022-03-29T09:12:38.073078Z","shell.execute_reply":"2022-03-29T09:12:38.078088Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# VGG-19","metadata":{}},{"cell_type":"code","source":"base_model= VGG19(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Dropout(0.5)(x)\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\npredictions = Activation('relu')(x)\n#x = Dropout(0.5)(x)\n#predictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(X_train_prep)\nval_features=model_feat.predict(X_val_prep)\ntest_features=model_feat.predict(X_test_prep)\n\nprint(train_features.shape)\nprint(val_features.shape)\nprint(test_features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:12:38.081425Z","iopub.execute_input":"2022-03-29T09:12:38.081961Z","iopub.status.idle":"2022-03-29T09:29:47.161209Z","shell.execute_reply.started":"2022-03-29T09:12:38.081893Z","shell.execute_reply":"2022-03-29T09:29:47.160145Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:29:47.169161Z","iopub.execute_input":"2022-03-29T09:29:47.169818Z","iopub.status.idle":"2022-03-29T09:29:47.176916Z","shell.execute_reply.started":"2022-03-29T09:29:47.169754Z","shell.execute_reply":"2022-03-29T09:29:47.174851Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# ResNet101","metadata":{}},{"cell_type":"code","source":"base_model= ResNet101(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\npredictions = Activation('relu')(x)\n#x = Dropout(0.5)(x)\n#predictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(X_train_prep)\nval_features=model_feat.predict(X_val_prep)\ntest_features=model_feat.predict(X_test_prep)\n\nprint(train_features.shape)\nprint(val_features.shape)\nprint(test_features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:29:47.182902Z","iopub.execute_input":"2022-03-29T09:29:47.183714Z","iopub.status.idle":"2022-03-29T09:38:22.126636Z","shell.execute_reply.started":"2022-03-29T09:29:47.183666Z","shell.execute_reply":"2022-03-29T09:38:22.125043Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:38:22.129081Z","iopub.execute_input":"2022-03-29T09:38:22.129575Z","iopub.status.idle":"2022-03-29T09:38:22.136483Z","shell.execute_reply.started":"2022-03-29T09:38:22.129541Z","shell.execute_reply":"2022-03-29T09:38:22.135623Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# MobileNet-V2","metadata":{}},{"cell_type":"code","source":"base_model= MobileNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\npredictions = Activation('relu')(x)\n#x = Dropout(0.5)(x)\n#predictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(X_train_prep)\nval_features=model_feat.predict(X_val_prep)\ntest_features=model_feat.predict(X_test_prep)\n\nprint(train_features.shape)\nprint(val_features.shape)\nprint(test_features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:38:22.137995Z","iopub.execute_input":"2022-03-29T09:38:22.138319Z","iopub.status.idle":"2022-03-29T09:39:50.763136Z","shell.execute_reply.started":"2022-03-29T09:38:22.138288Z","shell.execute_reply":"2022-03-29T09:39:50.761619Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:39:50.765818Z","iopub.execute_input":"2022-03-29T09:39:50.766351Z","iopub.status.idle":"2022-03-29T09:39:50.772874Z","shell.execute_reply.started":"2022-03-29T09:39:50.766302Z","shell.execute_reply":"2022-03-29T09:39:50.771320Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# MobileNet","metadata":{}},{"cell_type":"code","source":"base_model= MobileNet(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\npredictions = Activation('relu')(x)\n#x = Dropout(0.5)(x)\n#predictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(X_train_prep)\nval_features=model_feat.predict(X_val_prep)\ntest_features=model_feat.predict(X_test_prep)\n\nprint(train_features.shape)\nprint(val_features.shape)\nprint(test_features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:39:50.775069Z","iopub.execute_input":"2022-03-29T09:39:50.775904Z","iopub.status.idle":"2022-03-29T09:41:17.790067Z","shell.execute_reply.started":"2022-03-29T09:39:50.775855Z","shell.execute_reply":"2022-03-29T09:41:17.788963Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:41:17.791442Z","iopub.execute_input":"2022-03-29T09:41:17.792086Z","iopub.status.idle":"2022-03-29T09:41:17.797976Z","shell.execute_reply.started":"2022-03-29T09:41:17.792047Z","shell.execute_reply":"2022-03-29T09:41:17.796919Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# Inception-V3","metadata":{}},{"cell_type":"code","source":"base_model= InceptionV3(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\npredictions = Activation('relu')(x)\n#x = Dropout(0.5)(x)\n#predictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(X_train_prep)\nval_features=model_feat.predict(X_val_prep)\ntest_features=model_feat.predict(X_test_prep)\n\nprint(train_features.shape)\nprint(val_features.shape)\nprint(test_features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:41:17.799544Z","iopub.execute_input":"2022-03-29T09:41:17.800221Z","iopub.status.idle":"2022-03-29T09:44:11.856008Z","shell.execute_reply.started":"2022-03-29T09:41:17.800182Z","shell.execute_reply":"2022-03-29T09:44:11.854910Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:44:11.857410Z","iopub.execute_input":"2022-03-29T09:44:11.857723Z","iopub.status.idle":"2022-03-29T09:44:11.863223Z","shell.execute_reply.started":"2022-03-29T09:44:11.857686Z","shell.execute_reply":"2022-03-29T09:44:11.862270Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# InceptionResNet-V2","metadata":{}},{"cell_type":"code","source":"base_model= InceptionResNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\npredictions = Activation('relu')(x)\n#x = Dropout(0.5)(x)\n#predictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(X_train_prep)\nval_features=model_feat.predict(X_val_prep)\ntest_features=model_feat.predict(X_test_prep)\n\nprint(train_features.shape)\nprint(val_features.shape)\nprint(test_features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:44:11.864401Z","iopub.execute_input":"2022-03-29T09:44:11.864942Z","iopub.status.idle":"2022-03-29T09:50:46.036177Z","shell.execute_reply.started":"2022-03-29T09:44:11.864908Z","shell.execute_reply":"2022-03-29T09:50:46.034963Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:50:46.037669Z","iopub.execute_input":"2022-03-29T09:50:46.037968Z","iopub.status.idle":"2022-03-29T09:50:46.045595Z","shell.execute_reply.started":"2022-03-29T09:50:46.037940Z","shell.execute_reply":"2022-03-29T09:50:46.043945Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# DenseNet-169","metadata":{}},{"cell_type":"code","source":"base_model= DenseNet169(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\npredictions = Activation('relu')(x)\n#x = Dropout(0.5)(x)\n#predictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(X_train_prep)\nval_features=model_feat.predict(X_val_prep)\ntest_features=model_feat.predict(X_test_prep)\n\nprint(train_features.shape)\nprint(val_features.shape)\nprint(test_features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:50:46.047332Z","iopub.execute_input":"2022-03-29T09:50:46.047994Z","iopub.status.idle":"2022-03-29T09:56:23.271200Z","shell.execute_reply.started":"2022-03-29T09:50:46.047944Z","shell.execute_reply":"2022-03-29T09:56:23.270135Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:56:23.274123Z","iopub.execute_input":"2022-03-29T09:56:23.274491Z","iopub.status.idle":"2022-03-29T09:56:23.280321Z","shell.execute_reply.started":"2022-03-29T09:56:23.274461Z","shell.execute_reply":"2022-03-29T09:56:23.278888Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"# DenseNet-121","metadata":{}},{"cell_type":"code","source":"base_model= DenseNet121(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\npredictions = Activation('relu')(x)\n#x = Dropout(0.5)(x)\n#predictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(X_train_prep)\nval_features=model_feat.predict(X_val_prep)\ntest_features=model_feat.predict(X_test_prep)\n\nprint(train_features.shape)\nprint(val_features.shape)\nprint(test_features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:56:23.282195Z","iopub.execute_input":"2022-03-29T09:56:23.282904Z","iopub.status.idle":"2022-03-29T10:01:09.396289Z","shell.execute_reply.started":"2022-03-29T09:56:23.282856Z","shell.execute_reply":"2022-03-29T10:01:09.395296Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T10:01:09.397669Z","iopub.execute_input":"2022-03-29T10:01:09.398135Z","iopub.status.idle":"2022-03-29T10:01:09.402732Z","shell.execute_reply.started":"2022-03-29T10:01:09.398097Z","shell.execute_reply":"2022-03-29T10:01:09.401873Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"# XceptionNet","metadata":{}},{"cell_type":"code","source":"base_model= Xception(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\npredictions = Activation('relu')(x)\n#x = Dropout(0.5)(x)\n#predictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(X_train_prep)\nval_features=model_feat.predict(X_val_prep)\ntest_features=model_feat.predict(X_test_prep)\n\nprint(train_features.shape)\nprint(val_features.shape)\nprint(test_features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T10:01:09.405231Z","iopub.execute_input":"2022-03-29T10:01:09.405604Z","iopub.status.idle":"2022-03-29T10:07:31.413135Z","shell.execute_reply.started":"2022-03-29T10:01:09.405575Z","shell.execute_reply":"2022-03-29T10:07:31.411694Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T10:07:31.415093Z","iopub.execute_input":"2022-03-29T10:07:31.415430Z","iopub.status.idle":"2022-03-29T10:07:31.421252Z","shell.execute_reply.started":"2022-03-29T10:07:31.415398Z","shell.execute_reply":"2022-03-29T10:07:31.419928Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# clean up the space\n!rm -rf TRAIN TEST VAL TRAIN_CROP TEST_CROP VAL_CROP","metadata":{"execution":{"iopub.status.busy":"2022-03-29T10:07:31.423324Z","iopub.execute_input":"2022-03-29T10:07:31.424170Z","iopub.status.idle":"2022-03-29T10:07:32.743493Z","shell.execute_reply.started":"2022-03-29T10:07:31.424111Z","shell.execute_reply":"2022-03-29T10:07:32.742158Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}