{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow==1.14\n!pip install keras==2.2.5 \n!pip install 'h5py<3.0.0'","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:51:45.930711Z","iopub.execute_input":"2022-05-14T11:51:45.930965Z","iopub.status.idle":"2022-05-14T11:52:52.913812Z","shell.execute_reply.started":"2022-05-14T11:51:45.930937Z","shell.execute_reply":"2022-05-14T11:52:52.912959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output\n!pip install imutils\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:52:52.917362Z","iopub.execute_input":"2022-05-14T11:52:52.917586Z","iopub.status.idle":"2022-05-14T11:53:04.274199Z","shell.execute_reply.started":"2022-05-14T11:52:52.917561Z","shell.execute_reply":"2022-05-14T11:53:04.27339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nimport scipy\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.preprocessing.image import *\nfrom tensorflow.keras.utils import *\n# import pydot\n\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nimport tensorflow.keras.backend as K\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom colorama import Fore\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nfrom skimage.io import *\n%config Completer.use_jedi = False\nimport time\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nimport xgboost as xgb\nimport numpy as np \nfrom tqdm import tqdm\nimport cv2\nimport os\nimport shutil\nimport itertools\nimport imutils\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras import layers\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping\n\ninit_notebook_mode(connected=True)\nRANDOM_SEED = 123\n\nprint(\"All modules have been imported\")","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:04.275963Z","iopub.execute_input":"2022-05-14T11:53:04.276237Z","iopub.status.idle":"2022-05-14T11:53:10.41898Z","shell.execute_reply.started":"2022-05-14T11:53:04.276198Z","shell.execute_reply":"2022-05-14T11:53:10.418236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Add, Activation, Lambda\nfrom tensorflow.keras.layers import Conv2D\nfrom keras import backend as K\nfrom keras.activations import sigmoid\n\nfrom keras import optimizers\nfrom keras.optimizers import Adam\nimport keras.backend.tensorflow_backend as KTF\n#import keras.backend as KTF\nimport glob\nfrom keras.layers import Input,Dense,Dropout,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,concatenate,Activation,ZeroPadding2D\n#import tensorflow as tf\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport keras\nfrom keras.models import load_model\nfrom keras.layers import Activation, Dense\nfrom matplotlib import pyplot as plt\nfrom skimage import io,data\nimport time\nfrom keras import layers\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import regularizers\n\nfrom keras.preprocessing import image\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nnow = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime())\n\nimport os,sys\nos.getcwd()\n#os.chdir(\"/home/cjd/31_CNN_Attention\")\n#import os\n# \nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,5\"\n\n\n\n\n#import tensorflow as tf        \ndef focal_loss(gamma=2.):            \n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        return -K.sum( K.pow(1. - pt_1, gamma) * K.log(pt_1)) \n    return focal_loss_fixed\n\n\ndef Conv2d_BN(x, nb_filter,kernel_size, strides=(1,1), padding='same',name=None):  \n    if name is not None:  \n        bn_name = name + '_bn'  \n        conv_name = name + '_conv'  \n    else:  \n        bn_name = None  \n        conv_name = None  \n  \n    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x)  \n    x = BatchNormalization(axis=3,name=bn_name)(x)  \n    return x  \n\ndef Conv_Block(inpt,nb_filter,kernel_size,strides=(1,1), with_conv_shortcut=False):  \n    x = Conv2d_BN(inpt,nb_filter=nb_filter[0],kernel_size=(1,1),strides=strides,padding='same')  \n    x = Conv2d_BN(x, nb_filter=nb_filter[1], kernel_size=(3,3), padding='same')  \n    x = Conv2d_BN(x, nb_filter=nb_filter[2], kernel_size=(1,1), padding='same')  \n    if with_conv_shortcut:  \n        shortcut = Conv2d_BN(inpt,nb_filter=nb_filter[2],strides=strides,kernel_size=kernel_size)  \n        x = add([x,shortcut])  \n        return x  \n    else:  \n        x = add([x,inpt])  \n        return x  \n\n\ndef channel_attention(input_feature, ratio=8):\n\t\n\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n\tchannel = input_feature._keras_shape[channel_axis]\n\t\n\tshared_layer_one = Dense(channel//ratio,\n\t\t\t\t\t\t\t kernel_initializer='he_normal',\n\t\t\t\t\t\t\t activation = 'relu',\n\t\t\t\t\t\t\t use_bias=True,\n\t\t\t\t\t\t\t bias_initializer='zeros')\n\n\tshared_layer_two = Dense(channel,\n\t\t\t\t\t\t\t kernel_initializer='he_normal',\n\t\t\t\t\t\t\t use_bias=True,\n\t\t\t\t\t\t\t bias_initializer='zeros')\n\t\n\tavg_pool = GlobalAveragePooling2D()(input_feature)    \n\tavg_pool = Reshape((1,1,channel))(avg_pool)\n\tassert avg_pool._keras_shape[1:] == (1,1,channel)\n\tavg_pool = shared_layer_one(avg_pool)\n\tassert avg_pool._keras_shape[1:] == (1,1,channel//ratio)\n\tavg_pool = shared_layer_two(avg_pool)\n\tassert avg_pool._keras_shape[1:] == (1,1,channel)\n\t\n\tmax_pool = GlobalMaxPooling2D()(input_feature)\n\tmax_pool = Reshape((1,1,channel))(max_pool)\n\tassert max_pool._keras_shape[1:] == (1,1,channel)\n\tmax_pool = shared_layer_one(max_pool)\n\tassert max_pool._keras_shape[1:] == (1,1,channel//ratio)\n\tmax_pool = shared_layer_two(max_pool)\n\tassert max_pool._keras_shape[1:] == (1,1,channel)\n\t\n\tcbam_feature = Add()([avg_pool,max_pool])\n\tcbam_feature = Activation('hard_sigmoid')(cbam_feature)\n\t\n\tif K.image_data_format() == \"channels_first\":\n\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\t\n\treturn multiply([input_feature, cbam_feature])\n\n\ndef spatial_attention(input_feature):\n\tkernel_size = 7\n\tif K.image_data_format() == \"channels_first\":\n\t\tchannel = input_feature._keras_shape[1]\n\t\tcbam_feature = Permute((2,3,1))(input_feature)\n\telse:\n\t\tchannel = input_feature._keras_shape[-1]\n\t\tcbam_feature = input_feature\n\t\n\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n\tassert avg_pool._keras_shape[-1] == 1\n\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n\tassert max_pool._keras_shape[-1] == 1\n\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n\tassert concat._keras_shape[-1] == 2\n\tcbam_feature = Conv2D(filters = 1,\n\t\t\t\t\tkernel_size=kernel_size,\n\t\t\t\t\tactivation = 'hard_sigmoid',\n\t\t\t\t\tstrides=1,\n\t\t\t\t\tpadding='same',\n\t\t\t\t\tkernel_initializer='he_normal',\n\t\t\t\t\tuse_bias=False)(concat)\n\tassert cbam_feature._keras_shape[-1] == 1\n\t\n\tif K.image_data_format() == \"channels_first\":\n\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\t\t\n\treturn multiply([input_feature, cbam_feature])\n\n\ndef cbam_block(cbam_feature,ratio=8):\n\tcbam_feature = channel_attention(cbam_feature, ratio)\n\tcbam_feature = spatial_attention(cbam_feature, )\n\treturn cbam_feature\n\n\nIMG_SHAPE=(224, 224, 3)\n\nbase_model = keras.applications.MobileNetV2(input_shape=IMG_SHAPE,include_top=False, weights='imagenet')\n\n#weights='../working/cjd/01_rice_dete/obj_reco/checkpoint/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5'\nbase_model_layers_count=0\nfor layer in base_model.layers:\n    layer.trainable = False\n    base_model_layers_count=base_model_layers_count+1\nprint(\"MobileNetV2_base_model summary:\")\nprint(\"Number of layers in base_model:\")\nprint(base_model_layers_count)\nbase_model.summary()\n    \nbase_out = base_model.output\n\n#--------------------Soft attention module-------------------------------------------------------------- \nipts = base_out\nresidual = layers.Conv2D(1280, kernel_size = (1, 1), strides = (1, 1), padding = 'same')(ipts)\nresidual = layers.BatchNormalization(axis = -1)(residual)\ncbam = cbam_block(residual)\nbase_out = layers.add([base_out, residual, cbam])\n#------------------------------------------------------------------------------------------------------------ \nx = GlobalAveragePooling2D()(base_out)\n'''\nx = Dropout(0.2)(x)\nx = Dense(4096,activation=\"relu\")(x)\nx = Dense(4096,activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(2096,activation=\"relu\")(x)\n'''\n\n# softmax\n#predictions = Dense(len(ont_hot_labels[0]), activation='softmax', kernel_regularizer =regularizers.l2(0.01) )(x)  #l1_reg\nclasses=['idc-','idc+']\n#predictions = Dense(len(classes), activation='softmax', kernel_regularizer =regularizers.l2(0.01) )(x)  #l1_reg\npredictions = Dense(len(classes), activation='sigmoid', kernel_regularizer =regularizers.l2(0.01) )(x)  #l1_reg\n\n\n    \nfrom keras.models import Model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\nprint(\"Whole model summary:\")\nmodel.summary()\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy',  metrics = ['accuracy'])  #rmsprop\n#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adadelta(), metrics=['accuracy'])\n#model.compile(optimizer=optimizers.SGD(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy']) #loss='categorical_crossentropy',\n#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adadelta(), metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:58:44.68094Z","iopub.execute_input":"2022-05-14T11:58:44.681524Z","iopub.status.idle":"2022-05-14T11:58:52.840513Z","shell.execute_reply.started":"2022-05-14T11:58:44.681485Z","shell.execute_reply":"2022-05-14T11:58:52.839711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predictions = Dense(len(classes), activation='sigmoid', kernel_regularizer =regularizers.l2(0.01) )(x)  #l1_reg\n\nprint(len(classes))","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:18.604217Z","iopub.execute_input":"2022-05-14T11:53:18.60447Z","iopub.status.idle":"2022-05-14T11:53:18.612158Z","shell.execute_reply.started":"2022-05-14T11:53:18.604437Z","shell.execute_reply":"2022-05-14T11:53:18.611361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport keras\nprint(keras.__version__)\nimport tensorflow\nprint(keras.__version__)\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n!pip install Livelossplot\nfrom livelossplot import PlotLossesKeras\n\nfrom glob import glob\nimport os\nimport shutil\n\nimport numpy as np\nimport pandas as pd\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:18.613519Z","iopub.execute_input":"2022-05-14T11:53:18.613778Z","iopub.status.idle":"2022-05-14T11:53:28.065474Z","shell.execute_reply.started":"2022-05-14T11:53:18.613744Z","shell.execute_reply":"2022-05-14T11:53:28.064559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nimport scipy\n\n#import tensorflow as tf\nprint(keras.__version__)\n#print(tf.__version__)\nfrom keras.applications import *\nfrom keras.optimizers import *\nfrom keras.losses import *\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras.callbacks import *\nfrom keras.preprocessing.image import *\nfrom keras.utils import *\n# import pydot\n\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nimport tensorflow.keras.backend as K\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom colorama import Fore\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nfrom skimage.io import *\n%config Completer.use_jedi = False\nimport time\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nimport xgboost as xgb\n\nprint(\"All modules have been imported\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:28.07008Z","iopub.execute_input":"2022-05-14T11:53:28.072439Z","iopub.status.idle":"2022-05-14T11:53:28.113819Z","shell.execute_reply.started":"2022-05-14T11:53:28.072396Z","shell.execute_reply":"2022-05-14T11:53:28.111997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sampling_seed=0\n#size_4_training=100\n#img_size=224\n#training_reshape=(-1, img_size, img_size, 3)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:28.116181Z","iopub.execute_input":"2022-05-14T11:53:28.117054Z","iopub.status.idle":"2022-05-14T11:53:28.121197Z","shell.execute_reply.started":"2022-05-14T11:53:28.117015Z","shell.execute_reply":"2022-05-14T11:53:28.120233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nos.makedirs('../working/data/train_seg/idc-minus/')     \nos.makedirs('../working/data/train_seg/idc-plus/')  \nos.makedirs('../working/data/test_seg/idc-minus/')     \nos.makedirs('../working/data/test_seg/idc-plus/')  \nos.makedirs('../working/data/val_seg/idc-minus/')     \nos.makedirs('../working/data/val_seg/idc-plus/')  \n'''\n!apt-get install tree\nclear_output()\n# create new folders\n!mkdir TRAIN TEST VAL TRAIN/YES TRAIN/NO TEST/YES TEST/NO VAL/YES VAL/NO\n!tree -d\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:58:55.600493Z","iopub.execute_input":"2022-05-14T11:58:55.600926Z","iopub.status.idle":"2022-05-14T11:58:59.159947Z","shell.execute_reply.started":"2022-05-14T11:58:55.60089Z","shell.execute_reply":"2022-05-14T11:58:59.159086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_PATH = \"../input/brain-tumor-detection-mri/Brain_Tumor_Detection\"\n\n# split the data by train/val/test \nignored = {\"pred\"}\n# split the data by train/val/test\nfor CLASS in os.listdir(IMG_PATH):\n    if CLASS not in ignored:\n        if not CLASS.startswith('.'):\n            IMG_NUM = len(os.listdir(IMG_PATH +\"/\"+ CLASS))\n            for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH +\"/\"+ CLASS)):\n                img = IMG_PATH+ '/' +  CLASS + '/' + FILE_NAME\n                if n < 300:\n                    shutil.copy(img, 'TEST/' + CLASS.upper() + '/' + FILE_NAME)\n                elif n < 0.8*IMG_NUM:\n                    shutil.copy(img, 'TRAIN/'+ CLASS.upper() + '/' + FILE_NAME)\n                else:\n                    shutil.copy(img, 'VAL/'+ CLASS.upper() + '/' + FILE_NAME)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:31.349455Z","iopub.execute_input":"2022-05-14T11:53:31.350034Z","iopub.status.idle":"2022-05-14T11:53:46.28853Z","shell.execute_reply.started":"2022-05-14T11:53:31.35Z","shell.execute_reply":"2022-05-14T11:53:46.28776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(dir_path, img_size=(100,100)):\n    \"\"\"\n    Load resized images as np.arrays to workspace\n    \"\"\"\n    X = []\n    y = []\n    i = 0\n    labels = dict()\n    for path in tqdm(sorted(os.listdir(dir_path))):\n        if not path.startswith('.'):\n            labels[i] = path\n            for file in os.listdir(dir_path + path):\n                if not file.startswith('.'):\n                    img = cv2.imread(dir_path + path + '/' + file)\n                    X.append(img)\n                    y.append(i)\n            i += 1\n    X = np.array(X)\n    y = np.array(y)\n    print(f'{len(X)} images loaded from {dir_path} directory.')\n    return X, y, labels\n\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    cm = np.round(cm,2)\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:46.29145Z","iopub.execute_input":"2022-05-14T11:53:46.291704Z","iopub.status.idle":"2022-05-14T11:53:46.304283Z","shell.execute_reply.started":"2022-05-14T11:53:46.29167Z","shell.execute_reply":"2022-05-14T11:53:46.303366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport shutil\nfrom glob import glob \n#make directory for labelling","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:46.30805Z","iopub.execute_input":"2022-05-14T11:53:46.308261Z","iopub.status.idle":"2022-05-14T11:53:46.320836Z","shell.execute_reply.started":"2022-05-14T11:53:46.308231Z","shell.execute_reply":"2022-05-14T11:53:46.320095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_DIR = 'TRAIN/'\nTEST_DIR = 'TEST/'\nVAL_DIR = 'VAL/'\nIMG_SIZE = (224,224)\n\nprint('done')","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:46.322382Z","iopub.execute_input":"2022-05-14T11:53:46.322676Z","iopub.status.idle":"2022-05-14T11:53:46.333547Z","shell.execute_reply.started":"2022-05-14T11:53:46.322598Z","shell.execute_reply":"2022-05-14T11:53:46.332787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shutil.rmtree('./working/data')","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:46.334843Z","iopub.execute_input":"2022-05-14T11:53:46.335267Z","iopub.status.idle":"2022-05-14T11:53:46.340732Z","shell.execute_reply.started":"2022-05-14T11:53:46.335232Z","shell.execute_reply":"2022-05-14T11:53:46.340075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\nX_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\nX_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:46.342109Z","iopub.execute_input":"2022-05-14T11:53:46.342519Z","iopub.status.idle":"2022-05-14T11:53:51.614994Z","shell.execute_reply.started":"2022-05-14T11:53:46.342485Z","shell.execute_reply":"2022-05-14T11:53:51.614271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting the number of samples in Training, Validation and Test sets","metadata":{}},{"cell_type":"code","source":"#!pip install chart_studio\nimport plotly.graph_objs as go\n#from chart_studio.plotly import plot, iplot\nfrom plotly.offline import iplot\n#from plotly.plotly import iplot\nfrom plotly.subplots import make_subplots\n\ny = dict()\ny[0] = []\ny[1] = []\nfor set_name in (y_train, y_val, y_test):\n    y[0].append(np.sum(set_name == 0))\n    y[1].append(np.sum(set_name == 1))\n\ntrace0 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[0],\n    name='No',\n    marker=dict(color='#33cc33'),\n    opacity=0.7\n)\ntrace1 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[1],\n    name='Yes',\n    marker=dict(color='#ff3300'),\n    opacity=0.7\n)\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title='Count of classes in each set',\n    xaxis={'title': 'Set'},\n    yaxis={'title': 'Count'}\n)\nfig = go.Figure(data, layout)\niplot(fig)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:51.616515Z","iopub.execute_input":"2022-05-14T11:53:51.617051Z","iopub.status.idle":"2022-05-14T11:53:52.112863Z","shell.execute_reply.started":"2022-05-14T11:53:51.617012Z","shell.execute_reply":"2022-05-14T11:53:52.112161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's Visualize the images we are working with","metadata":{}},{"cell_type":"code","source":"def plot_samples(X, y, labels_dict, n=50):\n    \"\"\"\n    Creates a gridplot for desired number of images (n) from the specified set\n    \"\"\"\n    for index in range(len(labels_dict)):\n        imgs = X[np.argwhere(y == index)][:n]\n        j = 10\n        i = int(n/j)\n\n        plt.figure(figsize=(15,6))\n        c = 1\n        for img in imgs:\n            plt.subplot(i,j,c)\n            plt.imshow(img[0])\n\n            plt.xticks([])\n            plt.yticks([])\n            c += 1\n        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:52.113976Z","iopub.execute_input":"2022-05-14T11:53:52.114687Z","iopub.status.idle":"2022-05-14T11:53:52.12358Z","shell.execute_reply.started":"2022-05-14T11:53:52.114645Z","shell.execute_reply":"2022-05-14T11:53:52.122909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_samples(X_train, y_train, labels, 30)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:52.125004Z","iopub.execute_input":"2022-05-14T11:53:52.125255Z","iopub.status.idle":"2022-05-14T11:53:54.78577Z","shell.execute_reply.started":"2022-05-14T11:53:52.125222Z","shell.execute_reply":"2022-05-14T11:53:54.785153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Cropping the images </h2>\n\n<p> The images we have are of different sizes. But our model accepts images of size (224*224*3) as input. To achienve this ew have to resize the images. Blindly resizing the images can lead to extreme distortions in the images. Hence, We will first crop thie images and then resize them. This will minimize the issue of distortions. </p>\n\n<p> This cropping is done by finding contours in the images using the OpenCV Library </p>","metadata":{}},{"cell_type":"code","source":"def crop_imgs(set_name, add_pixels_value=0):\n    \"\"\"\n    Finds the extreme points on the image and crops the rectangular out of them\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # threshold the image, then perform a series of erosions +\n        # dilations to remove any small regions of noise\n        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n        thresh = cv2.erode(thresh, None, iterations=2)\n        thresh = cv2.dilate(thresh, None, iterations=2)\n\n        # find contours in thresholded image, then grab the largest one\n        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts = imutils.grab_contours(cnts)\n        c = max(cnts, key=cv2.contourArea)\n\n        # find the extreme points\n        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n        extRight = tuple(c[c[:, :, 0].argmax()][0])\n        extTop = tuple(c[c[:, :, 1].argmin()][0])\n        extBot = tuple(c[c[:, :, 1].argmax()][0])\n\n        ADD_PIXELS = add_pixels_value\n        new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n        set_new.append(new_img)\n\n    return np.array(set_new)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:54.786977Z","iopub.execute_input":"2022-05-14T11:53:54.787771Z","iopub.status.idle":"2022-05-14T11:53:54.79943Z","shell.execute_reply.started":"2022-05-14T11:53:54.787733Z","shell.execute_reply":"2022-05-14T11:53:54.798639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imutils\nimg = cv2.imread('./VAL/NO/no852.jpg')\nimg = cv2.resize(\n            img,\n            dsize=IMG_SIZE,\n            interpolation=cv2.INTER_CUBIC\n        )\ngray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\ngray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n# threshold the image, then perform a series of erosions +\n# dilations to remove any small regions of noise\nthresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\nthresh = cv2.erode(thresh, None, iterations=2)\nthresh = cv2.dilate(thresh, None, iterations=2)\n\n# find contours in thresholded image, then grab the largest one\ncnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\nc = max(cnts, key=cv2.contourArea)\n\n# find the extreme points\nextLeft = tuple(c[c[:, :, 0].argmin()][0])\nextRight = tuple(c[c[:, :, 0].argmax()][0])\nextTop = tuple(c[c[:, :, 1].argmin()][0])\nextBot = tuple(c[c[:, :, 1].argmax()][0])\n\n# add contour on the image\nimg_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n\n# add extreme points\nimg_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\nimg_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n\n# crop\nADD_PIXELS = 0\nnew_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:54.800708Z","iopub.execute_input":"2022-05-14T11:53:54.801031Z","iopub.status.idle":"2022-05-14T11:53:54.864379Z","shell.execute_reply.started":"2022-05-14T11:53:54.800993Z","shell.execute_reply":"2022-05-14T11:53:54.863723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Let's visualize how the cropping works </h2>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.subplot(141)\nplt.imshow(img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 1. Get the original image')\nplt.subplot(142)\nplt.imshow(img_cnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 2. Find the biggest contour')\nplt.subplot(143)\nplt.imshow(img_pnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 3. Find the extreme points')\nplt.subplot(144)\nplt.imshow(new_img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 4. Crop the image')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:54.865377Z","iopub.execute_input":"2022-05-14T11:53:54.865598Z","iopub.status.idle":"2022-05-14T11:53:55.113609Z","shell.execute_reply.started":"2022-05-14T11:53:54.865566Z","shell.execute_reply":"2022-05-14T11:53:55.112707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_crop = crop_imgs(set_name=X_train)\nX_val_crop = crop_imgs(set_name=X_val)\nX_test_crop = crop_imgs(set_name=X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:55.115113Z","iopub.execute_input":"2022-05-14T11:53:55.115415Z","iopub.status.idle":"2022-05-14T11:53:58.004461Z","shell.execute_reply.started":"2022-05-14T11:53:55.115381Z","shell.execute_reply":"2022-05-14T11:53:58.003667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Let's visualize the images after being cropped </h2>","metadata":{}},{"cell_type":"code","source":"plot_samples(X_train_crop, y_train, labels, 30)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:53:58.005652Z","iopub.execute_input":"2022-05-14T11:53:58.006041Z","iopub.status.idle":"2022-05-14T11:54:01.071074Z","shell.execute_reply.started":"2022-05-14T11:53:58.006008Z","shell.execute_reply":"2022-05-14T11:54:01.07022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_new_images(x_set, y_set, folder_name):\n    i = 0\n    for (img, imclass) in zip(x_set, y_set):\n        if imclass == 0:\n            cv2.imwrite(folder_name+'NO/'+str(i)+'.jpg', img)\n        else:\n            cv2.imwrite(folder_name+'YES/'+str(i)+'.jpg', img)\n        i += 1","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:54:01.07238Z","iopub.execute_input":"2022-05-14T11:54:01.072717Z","iopub.status.idle":"2022-05-14T11:54:01.081758Z","shell.execute_reply.started":"2022-05-14T11:54:01.072681Z","shell.execute_reply":"2022-05-14T11:54:01.080826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saving new images to the folder\n!mkdir TRAIN_CROP TEST_CROP VAL_CROP TRAIN_CROP/YES TRAIN_CROP/NO TEST_CROP/YES TEST_CROP/NO VAL_CROP/YES VAL_CROP/NO\n\nsave_new_images(X_train_crop, y_train, folder_name='TRAIN_CROP/')\nsave_new_images(X_val_crop, y_val, folder_name='VAL_CROP/')\nsave_new_images(X_test_crop, y_test, folder_name='TEST_CROP/')","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:54:01.082963Z","iopub.execute_input":"2022-05-14T11:54:01.083366Z","iopub.status.idle":"2022-05-14T11:54:08.073146Z","shell.execute_reply.started":"2022-05-14T11:54:01.083329Z","shell.execute_reply":"2022-05-14T11:54:08.072094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Resizing the images </h2>\n\n<p> Now that we have cropped the images, we can resize them without suffering from heavy distortions or resizing artefacts </p>","metadata":{}},{"cell_type":"code","source":"def preprocess_imgs(set_name, img_size):\n    set_new = []\n    for img in set_name:\n        img = cv2.resize(\n            img,\n            dsize=img_size,\n            interpolation=cv2.INTER_CUBIC\n        )\n        set_new.append(preprocess_input(img))\n    return np.array(set_new)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:54:08.075096Z","iopub.execute_input":"2022-05-14T11:54:08.075392Z","iopub.status.idle":"2022-05-14T11:54:08.08238Z","shell.execute_reply.started":"2022-05-14T11:54:08.075357Z","shell.execute_reply":"2022-05-14T11:54:08.080602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_prep = preprocess_imgs(set_name=X_train_crop, img_size=IMG_SIZE)\nX_test_prep = preprocess_imgs(set_name=X_test_crop, img_size=IMG_SIZE)\nX_val_prep = preprocess_imgs(set_name=X_val_crop, img_size=IMG_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:54:08.083666Z","iopub.execute_input":"2022-05-14T11:54:08.08391Z","iopub.status.idle":"2022-05-14T11:54:11.950601Z","shell.execute_reply.started":"2022-05-14T11:54:08.083873Z","shell.execute_reply":"2022-05-14T11:54:11.949298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_samples(X_train_prep, y_train, labels, 30)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T11:54:11.958761Z","iopub.execute_input":"2022-05-14T11:54:11.961105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    rescale=1./255,\n    shear_range=0.05,\n    brightness_range=[0.1, 1.5],\n    horizontal_flip=True,\n    vertical_flip=True\n)\n\nos.mkdir('preview')\nx = X_train_crop[0]  \nx = x.reshape((1,) + x.shape) \n\ni = 0\nfor batch in demo_datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='aug_img', save_format='jpg'):\n    i += 1\n    if i > 20:\n        break \n        \nplt.imshow(X_train_crop[0])\nplt.xticks([])\nplt.yticks([])\nplt.title('Original Image')\nplt.show()\n\nplt.figure(figsize=(15,6))\ni = 1\nfor img in os.listdir('preview/'):\n    img = cv2.cv2.imread('preview/' + img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.subplot(3,7,i)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    i += 1\n    if i > 3*7:\n        break\nplt.suptitle('Augemented Images')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf preview/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_DIR = 'TRAIN_CROP/'\nVAL_DIR = 'VAL_CROP/'\nRANDOM_SEED = 42\ntrain_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    brightness_range=[0.5, 1.5],\n    horizontal_flip=True,\n    vertical_flip=True,\n    preprocessing_function=preprocess_input\n)\n\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input\n)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=32,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)\n\n\nvalidation_generator = test_datagen.flow_from_directory(\n    VAL_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=16,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**load gen**","metadata":{}},{"cell_type":"code","source":"\ntrain_datagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\ntest_datagen = ImageDataGenerator(rescale=1./255)\ntrain_generator = train_datagen.flow_from_directory(\n        'TRAIN_CROP/',   \n        target_size=(224, 224),\n        batch_size=32,\n        class_mode='categorical')\nvalidation_generator = test_datagen.flow_from_directory(\n         'VAL_CROP/',\n        target_size=(224, 224),\n        batch_size=32,\n         class_mode='categorical')\nmodel.fit(\n        train_generator,\n        steps_per_epoch=50,\n        epochs=30,\n        validation_data=validation_generator,\n        validation_steps=25)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T12:10:19.217807Z","iopub.execute_input":"2022-05-14T12:10:19.218608Z","iopub.status.idle":"2022-05-14T14:21:40.121597Z","shell.execute_reply.started":"2022-05-14T12:10:19.218553Z","shell.execute_reply":"2022-05-14T14:21:40.120816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\n#print(os.getcwd())\n#print (sys.version)\n#os.makedirs('../working/log/')    \n\n\n\n\n\n\n\n\n\nimport os\n# \nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,5\"\n\n#import tensorflow as tf   \n#config = tf.ConfigProto()\n#config.gpu_options.allow_growth = True\n#keras.backend.tensorflow_backend.set_session(tf.Session(config=config))\n\n\n     \ndef focal_loss(gamma=2.):            \n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        return -K.sum( K.pow(1. - pt_1, gamma) * K.log(pt_1)) \n    return focal_loss_fixed\n\n\ndef Conv2d_BN(x, nb_filter,kernel_size, strides=(1,1), padding='same',name=None):  \n    if name is not None:  \n        bn_name = name + '_bn'  \n        conv_name = name + '_conv'  \n    else:  \n        bn_name = None  \n        conv_name = None  \n  \n    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x)  \n    x = BatchNormalization(axis=3,name=bn_name)(x)  \n    return x  \n\ndef Conv_Block(inpt,nb_filter,kernel_size,strides=(1,1), with_conv_shortcut=False):  \n    x = Conv2d_BN(inpt,nb_filter=nb_filter[0],kernel_size=(1,1),strides=strides,padding='same')  \n    x = Conv2d_BN(x, nb_filter=nb_filter[1], kernel_size=(3,3), padding='same')  \n    x = Conv2d_BN(x, nb_filter=nb_filter[2], kernel_size=(1,1), padding='same')  \n    if with_conv_shortcut:  \n        shortcut = Conv2d_BN(inpt,nb_filter=nb_filter[2],strides=strides,kernel_size=kernel_size)  \n        x = add([x,shortcut])  \n        return x  \n    else:  \n        x = add([x,inpt])  \n        return x  \n\n\ndef channel_attention(input_feature, ratio=8):\n\t\n\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n\tchannel = input_feature.shape[channel_axis]\n\t\n\tshared_layer_one = Dense(channel//ratio,\n\t\t\t\t\t\t\t kernel_initializer='he_normal',\n\t\t\t\t\t\t\t activation = 'relu',\n\t\t\t\t\t\t\t use_bias=True,\n\t\t\t\t\t\t\t bias_initializer='zeros')\n\n\tshared_layer_two = Dense(channel,\n\t\t\t\t\t\t\t kernel_initializer='he_normal',\n\t\t\t\t\t\t\t use_bias=True,\n\t\t\t\t\t\t\t bias_initializer='zeros')\n\t\n\tavg_pool = GlobalAveragePooling2D()(input_feature)    \n\tavg_pool = Reshape((1,1,channel))(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel)\n\tavg_pool = shared_layer_one(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n\tavg_pool = shared_layer_two(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel)\n\t\n\tmax_pool = GlobalMaxPooling2D()(input_feature)\n\tmax_pool = Reshape((1,1,channel))(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel)\n\tmax_pool = shared_layer_one(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n\tmax_pool = shared_layer_two(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel)\n\t\n\tcbam_feature = Add()([avg_pool,max_pool])\n\tcbam_feature = Activation('hard_sigmoid')(cbam_feature)\n\t\n\tif K.image_data_format() == \"channels_first\":\n\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\t\n\treturn multiply([input_feature, cbam_feature])\n\n\ndef spatial_attention(input_feature):\n\tkernel_size = 7\n\tif K.image_data_format() == \"channels_first\":\n\t\tchannel = input_feature.shape[1]\n\t\tcbam_feature = Permute((2,3,1))(input_feature)\n\telse:\n\t\tchannel = input_feature.shape[-1]\n\t\tcbam_feature = input_feature\n\t\n\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n\tassert avg_pool.shape[-1] == 1\n\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n\tassert max_pool.shape[-1] == 1\n\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n\tassert concat.shape[-1] == 2\n\tcbam_feature = Conv2D(filters = 1,\n\t\t\t\t\tkernel_size=kernel_size,\n\t\t\t\t\tactivation = 'hard_sigmoid',\n\t\t\t\t\tstrides=1,\n\t\t\t\t\tpadding='same',\n\t\t\t\t\tkernel_initializer='he_normal',\n\t\t\t\t\tuse_bias=False)(concat)\n\tassert cbam_feature.shape[-1] == 1\n\t\n\tif K.image_data_format() == \"channels_first\":\n\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\t\t\n\treturn multiply([input_feature, cbam_feature])\n\n\ndef cbam_block(cbam_feature,ratio=8):\n\tcbam_feature = channel_attention(cbam_feature, ratio)\n\tcbam_feature = spatial_attention(cbam_feature, )\n\treturn cbam_feature\n\n\n#batch_size = 64 \n#epochs = 30\n\n\n#board_name1 = '../working/log/stage1/' + now + '/'\n#board_name2 = '../working/log/stage2/' + now + '/'\n\nimg_size = (224, 224)  \n#classes=list(range(1,5))\n#classes=['1','2','3','4']\nimport os\nimport glob\n#nb_train_samples = len(glob.glob('TRAIN/' + '/*/*.*'))    #./TRAIN\n#nb_validation_samples = len(glob.glob('VAL/' + '/*/*.*'))  \n\n#classes = sorted([o for o in os.listdir('./TRAIN')])  \n#print(classes)\n\n\n#---------Attention embedded MobileNetV2--------------------------------------------------------------\n\n\nIMG_SHAPE=(224, 224, 3)\n\n#base_model = keras.applications.MobileNetV2(input_shape=IMG_SHAPE,include_top=False, weights='imagenet')\n#weights='../working/cjd/01_rice_dete/obj_reco/checkpoint/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5')\n\n\n\n#for layer in base_model.layers:\n#    layer.trainable = False\n    \n\n#base_out = base_model.output\n\n#--------------------Soft attention module-------------------------------------------------------------- \n#ipts = base_out\n#residual = layers.Conv2D(1280, kernel_size = (1, 1), strides = (1, 1), padding = 'same')(ipts)\n#residual = layers.BatchNormalization(axis = -1)(residual)\n#cbam = cbam_block(residual)\n#base_out = layers.add([base_out, residual, cbam])\n#------------------------------------------------------------------------------------------------------------ \n\n#x = GlobalAveragePooling2D()(base_out)\n# \n\n# softmax\n#predictions = Dense(len(ont_hot_labels[0]), activation='softmax', kernel_regularizer =regularizers.l2(0.01) )(x)  #l1_reg\n#predictions = Dense(len(classes), activation='softmax', kernel_regularizer =regularizers.l2(0.01) )(x)  #l1_reg\n\n#model = Model(inputs=base_model.input, outputs=predictions)\n\n#model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics = ['accuracy'])  #rmsprop\n#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adadelta(), metrics=['accuracy'])\n\n\n\n#train_datagen = ImageDataGenerator(validation_split=0.2)\n#train_datagen = ImageDataGenerator(./TRAIN)\n#train_datagen.mean = np.array([103.939, 116.779, 123.68], dtype=np.float32).reshape((3, 1, 1))  # remove imagenet BGR mean value\n#train_datagen = train_datagen.flow_from_directory('TRAIN_DIR', target_size=img_size, classes=classes)\n\n#train_generator= './TRAIN'\n\n\n\n#VAL_DIR = 'VAL/\n\nvalidation_datagen = ImageDataGenerator()\n#validation_datagen.mean = np.array([103.939, 116.779, 123.68], dtype=np.float32).reshape((3, 1, 1))\n#test_dir = './TEST'  \n#validation_data = validation_datagen.flow_from_directory(test_dir, target_size=img_size, classes=classes)\n#validation_generator = './VAL'\n\n#model_checkpoint1 = ModelCheckpoint(filepath=MODEL_INIT, save_best_only=True, monitor='val_accuracy', mode='max')\n#model_checkpoint1 = ModelCheckpoint(filepath=MODEL_INIT, monitor='val_accuracy', save_best_only=True, monitor='val_accuracy', mode='max')\n#model_checkpoint1 = ModelCheckpoint(filepath=MODEL_INIT, monitor='val_accuracy', save_best_only=True, mode='max')\n'''board1 = TensorBoard(log_dir=board_name1,\n                     histogram_freq=0,\n                     write_graph=True,\n                     write_images=True)'''\n#callback_list1 = [model_checkpoint1, board1] \n\n\n#MODEL_INIT = '../working/log/init_model.h5'\n#MODEL_PATH = '../working/log/tst_model.h5'\n#callbacks1 = [ModelCheckpoint('init_model.hdf5', save_best_only=True), TensorBoard(log_dir=board_name1, histogram_freq=0,write_graph=True, write_images=True)]\n#callbacks1 = [ModelCheckpoint('init_model.hdf5', save_best_only=True), TensorBoard(log_dir=board_name1, histogram_freq=0,write_graph=True, write_images=True)]\n'''model.fit_generator(train_data, steps_per_epoch=nb_train_samples / float(batch_size),\n                           epochs = epochs,nb_train_samples = len(glob.glob('TRAIN' + '/*/*.*')),\n                           validation_steps=nb_validation_samples / float(batch_size),\n''''''\nEPOCHS = 30\nes = EarlyStopping(\n    monitor='val_acc', \n    mode='max',\n    patience=6\n)\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=50,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=25,\n    callbacks=[es]\n)\n'''                          ","metadata":{"execution":{"iopub.status.busy":"2022-05-14T16:33:15.273032Z","iopub.execute_input":"2022-05-14T16:33:15.273312Z","iopub.status.idle":"2022-05-14T16:33:15.318446Z","shell.execute_reply.started":"2022-05-14T16:33:15.27328Z","shell.execute_reply":"2022-05-14T16:33:15.317708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.load_weights('./init_model.hdf5')","metadata":{"execution":{"iopub.status.busy":"2022-05-14T16:38:01.07823Z","iopub.execute_input":"2022-05-14T16:38:01.078491Z","iopub.status.idle":"2022-05-14T16:38:01.102647Z","shell.execute_reply.started":"2022-05-14T16:38:01.078461Z","shell.execute_reply":"2022-05-14T16:38:01.101301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#---------------2-nd stage---------------------------------------------\n#model_checkpoint2 = ModelCheckpoint(filepath=MODEL_PATH,  monitor='val_accuracy')\n#model_checkpoint2 = ModelCheckpoint('tst_model.h5',  monitor='val_accuracy', save_best_only=True, mode='max')\n#board2 = TensorBoard(log_dir=board_name2, histogram_freq=0, write_graph=True, write_images=True)\n#callback_list2 = [model_checkpoint2, board2]\n#callbacks2 = [EarlyStopping(monitor='val_loss', patience=5, verbose=2), ModelCheckpoint('test_model.hdf5', save_best_only=True), TensorBoard(log_dir=board_name2,\n#                     histogram_freq=0,\n#                     write_graph=True,\n#                     write_images=True)]\n'''callbacks2 = [ModelCheckpoint('test_model.hdf5', save_best_only=True), TensorBoard(log_dir=board_name2,\n                     histogram_freq=0,\n                     write_graph=True,\n                     write_images=True)]'''\n'''\n#model.load_weights(MODEL_INIT)\nfor model1 in model.layers:\n    model1.trainable = True\n#fine_tune_at = 50\n#for layer in model.layers[:fine_tune_at]:\n#    layer.trainable = False\n\n'''\n#model.compile(optimizer=optimizers.Adam(), loss =[focal_loss(gamma=2)], metrics=['accuracy']) #loss='categorical_crossentropy',\n#model.compile(optimizer=optimizers.SGD(lr=0.0001), loss = [focal_loss(gamma=2)], metrics=['accuracy']) #loss='categorical_crossentropy',\n#model.compile(optimizer=optimizers.Adadelta(), loss = [focal_loss(gamma=2)], metrics=['accuracy']) #loss='categorical_crossentropy',\n\n#model.fit_generator(train_data, steps_per_epoch=nb_train_samples / float(batch_size), epochs=epochs, validation_data=validation_data, validation_steps=nb_validation_samples / float(batch_size),verbose=2)\n\n\n''''\nfrom contextlib import redirect_stdout   \nwith open('./model_summary.txt', 'w') as f:\n    with redirect_stdout(f):\n        model.summary(line_length=200,positions=[0.30,0.60,0.7,1.0])\n\n\n'''\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T16:35:22.529775Z","iopub.execute_input":"2022-05-14T16:35:22.530423Z","iopub.status.idle":"2022-05-14T16:35:22.536966Z","shell.execute_reply.started":"2022-05-14T16:35:22.530383Z","shell.execute_reply":"2022-05-14T16:35:22.536157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, roc_auc_score, cohen_kappa_score, precision_score, recall_score, accuracy_score, confusion_matrix\ndef get_accuracy_metrics(model, X_train, y_train, X_val, y_val, X_test, y_test):\n    y_train=np.argmax(y_train, axis=1)\n    y_test=np.argmax(y_test, axis=1)\n    y_val=np.argmax(y_val, axis=1)\n    y_train_pred=np.argmax(model.predict(X_train),axis=1)\n    y_test_pred=np.argmax(model.predict(X_test),axis=1)\n    y_val_pred=np.argmax(model.predict(X_val),axis=1)\n    print(\"Train accuracy Score------------>\")\n    print (\"{0:.3f}\".format(accuracy_score(y_train, y_train_pred)*100), \"%\")\n    \n    print(\"Val accuracy Score--------->\")\n    \n    print(\"{0:.3f}\".format(accuracy_score(y_val, y_val_pred)*100), \"%\")\n    \n  \n    print(\"Test accuracy Score--------->\")\n    print(\"{0:.3f}\".format(accuracy_score(y_test, y_test_pred)*100), \"%\")\n    \n    print(\"F1 Score--------------->\")\n    print(\"{0:.3f}\".format(f1_score(y_test, y_test_pred, average = 'weighted')*100), \"%\")\n    \n    print(\"Cohen Kappa Score------------->\")\n    print(\"{0:.3f}\".format(cohen_kappa_score(y_test, y_test_pred)*100), \"%\")\n    \n    print(\"Recall-------------->\")\n    print(\"{0:.3f}\".format(recall_score(y_test, y_test_pred, average = 'weighted')*100), \"%\")\n    \n    print(\"Precision-------------->\")\n    print(\"{0:.3f}\".format(precision_score(y_test, y_test_pred, average = 'weighted')*100), \"%\")\n    \n    cf_matrix_test = confusion_matrix(y_test, y_test_pred)\n    cf_matrix_val = confusion_matrix(y_val, y_val_pred)\n    \n    plt.figure(figsize = (12, 6))\n    plt.subplot(121)\n    sns.heatmap(cf_matrix_val, annot=True, cmap='Blues')\n    plt.title(\"Val Confusion matrix\")\n    \n    plt.subplot(122)\n    sns.heatmap(cf_matrix_test, annot=True, cmap='Blues')\n    plt.title(\"Test Confusion matrix\")\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T16:36:31.724483Z","iopub.execute_input":"2022-05-14T16:36:31.725047Z","iopub.status.idle":"2022-05-14T16:36:31.736558Z","shell.execute_reply.started":"2022-05-14T16:36:31.725003Z","shell.execute_reply":"2022-05-14T16:36:31.735789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MODEL_PATH = './test_model.hdf5'\n#model.load_weights(MODEL_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_accuracy_metrics(model,  X_train, y_train, X_val, y_val, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T16:35:46.868151Z","iopub.execute_input":"2022-05-14T16:35:46.868957Z","iopub.status.idle":"2022-05-14T16:35:46.893433Z","shell.execute_reply.started":"2022-05-14T16:35:46.868917Z","shell.execute_reply":"2022-05-14T16:35:46.892619Z"},"trusted":true},"execution_count":null,"outputs":[]}]}