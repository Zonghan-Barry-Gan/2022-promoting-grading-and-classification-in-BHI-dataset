{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Installs\n!pip install -U fvcore\n# !pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n\n# Imports and Initialize\nimport os\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statistics import mean\n\n#usrlibs\nfrom modelhistory import ModelHistory\nfrom howlong import HowLong\n\n#torch\nimport torch \nimport torch.nn as nn\nfrom torch.nn import Conv2d #, ChannelShuffle\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\ntorch.manual_seed(0)\ntorch.backends.cudnn.benchmark = False\n\ntotal_howlong = HowLong()\n\nDIM = 512 # SET IMAGE DIMENSION ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyper parameters\nnum_epochs = 1\nnum_classes = 2\nbatch_size = 10\nlearning_rate = 0.0005\n\n# Device configuration\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lines = []\ntrain_data=[]\ntest_data=[]\nwith open('input/covidx-cxr2/train.txt') as f:\n    lines = f.readlines()\n\ncount = 0\nfor line in lines:\n    l=line.split()\n    label=0\n    if (l[2]=='positive'):\n        label=1\n    train_data.append([\"input/covidx-cxr2/train/\"+l[1],label])\n\nlines = []\nwith open('input/covidx-cxr2/test.txt') as f:\n    lines = f.readlines()\n\ncount = 0\nfor line in lines:\n    l=line.split()\n    label=0\n    if (l[2]=='positive'):\n        label=1\n    test_data.append([\"../input/covidx-cxr2/test/\"+l[1],label])\n\nimages_df = pd.DataFrame(data=train_data, columns=[\"images\", \"labels\"])\nprint(images_df.head(10))\nimages_df.groupby('labels').size()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# handling imbalance\n# create copies of the imbalanced class\npositive_df = images_df[images_df['labels'] == 1]\nframes = [images_df, positive_df,positive_df,positive_df,positive_df,positive_df]\nimages_df = pd.concat(frames)\nimages_df.groupby('labels').size()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.DataFrame(data=test_data, columns=[\"images\", \"labels\"])\ntest.groupby('labels').size()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, val = train_test_split(images_df, stratify=images_df.labels, test_size=0.015)\nlen(train),  len(val), len(test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, df_data,transform=None):\n        super().__init__()\n        self.df = df_data.values\n        \n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path,label = self.df[index]\n        \n        image = cv2.imread(img_path)\n        image = cv2.resize(image, (DIM,DIM))\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = ModelHistory()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trans_train = transforms.Compose([transforms.ToPILImage(),\n                                  transforms.Pad(64, padding_mode='reflect'),\n                                  transforms.RandomHorizontalFlip(), \n                                  transforms.RandomVerticalFlip(),\n                                  transforms.RandomRotation(20), \n                                  transforms.Resize(DIM, interpolation = 2),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n\ntrans_valid = transforms.Compose([transforms.ToPILImage(),                    \n                                  transforms.Pad(64, padding_mode='reflect'),\n                                  transforms.Resize(DIM, interpolation = 2),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n\ndataset_train = MyDataset(df_data=train, transform=trans_train)\ndataset_valid = MyDataset(df_data=val,transform=trans_valid)\ndataset_test = MyDataset(df_data=test,transform=trans_valid)\n\nloader_train = DataLoader(dataset = dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\nloader_valid = DataLoader(dataset = dataset_valid, batch_size=batch_size//2, shuffle=False, num_workers=0)\nloader_test = DataLoader(dataset = dataset_test, batch_size=batch_size//2, shuffle=False, num_workers=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FocalLoss(nn.modules.loss._WeightedLoss):\n    def __init__(self, weight=None, gamma=2,reduction='mean'):\n        super(FocalLoss, self).__init__(weight,reduction=reduction)\n        self.gamma = gamma\n        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n\n    def forward(self, input, target):\n\n        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n        pt = torch.exp(-ce_loss)\n        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n        return focal_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch.nn.modules.utils import _single, _pair, _triple\nimport math\n\nclass CustomConv(torch.nn.Module):\n    def __init__(self, in_channels, out_channels,kernel_size=3, dilation=2, padding=0, stride=2, bias=True, mu=0.1):\n        super(CustomConv,self).__init__()\n        device = \"cpu\"\n        if(torch.cuda.is_available()):\n            device = 'cuda'\n            \n        self.kernel_size=_pair(kernel_size)\n        self.out_channels=out_channels\n        self.dilation=_pair(dilation)\n        self.padding=_pair(padding)\n        self.stride=_pair(stride)\n        self.in_channels=in_channels\n        self.mu=mu\n        self.mu_=(1-mu)/3\n        self.bias1=torch.nn.Parameter(torch.Tensor(out_channels))\n        self.device = device\n        \n        #self.bias1=self.bias1.to(device)\n        mu_=self.mu_\n        self.calculated_kernel_size=self.dilation[0]*(self.kernel_size[0]-1)+1\n        self.weight=torch.nn.Parameter(torch.Tensor(self.out_channels,self.in_channels,self.kernel_size[0],self.kernel_size[1]))\n\n        self.fuz=self.mask_dial(self.kernel_size[0],self.dilation[0],self.mu)\n        self.fuz[self.fuz==1] = 0\n        self.fuz=self.fuz.unsqueeze(0)#.unsqueeze(0).unsqueeze(0)\n        \n        temp=self.fuz\n        for i in range(1,self.in_channels):\n            temp=torch.cat((temp,self.fuz))\n        temp=temp.unsqueeze(0)\n        temp1=temp\n        for i in range(1,self.out_channels):\n            temp1=torch.cat((temp1,temp))\n        self.fuz=temp1\n        if(bias):\n            self.bias=torch.nn.Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameters(\"bias\",None)\n        self.fuz=self.fuz.to(self.device)\n        self.reset_parameters()\n    \n    def mask_dial(self,kernel_size,dilation,mu):\n        dilation-=1\n        mid=[0 for i in range(dilation)]\n        lim=(dilation//2) if (dilation%2==0) else ((dilation//2)+1)\n        diff=(1-mu)/lim\n        filter1=[]\n        for i in range(lim):\n            mid[i]=1-(i+1)*diff\n            mid[dilation-1-i]=1-(i+1)*diff\n        for i in range(2*kernel_size-1):\n            if(i%2==0):\n                filter1=filter1+[0]\n            else:\n                filter1=filter1+mid\n        filter2=[[0 for i in range(dilation+2)] for j in range(dilation)]\n        for i in range(lim):\n            for j in range(i+2):\n                filter2[i][j]=mid[i]\n                filter2[i][dilation+1-j]=mid[i]\n                filter2[dilation-i-1][j]=mid[i]\n                filter2[dilation-i-1][dilation+1-j]=mid[i]\n            for j in range(i+1,lim):\n                filter2[i][j+1]=mid[j]\n                filter2[i][dilation-j]=mid[j]\n                filter2[dilation-i-1][j+1]=mid[j]\n                filter2[dilation-i-1][dilation-j]=mid[j]\n        filter3=[x[1:] for x in filter2]\n        for i in range(kernel_size-2):\n            for j in range(len(filter2)):\n                filter2[j]+=filter3[j]\n        result=[]\n        for i in range(2*kernel_size-1):\n            if(i%2==0):\n                result=result+[filter1]\n            else:\n                result=result+filter2\n                result=[0 for i in range(2*kernel_size-1)]\n        result=[]\n        for i in range(2*kernel_size-1):\n            if(i%2==0):\n                result=result+[filter1]\n            else:\n                result=result+filter2\n        result=torch.Tensor(result)\n        return result\n\n    \n    def reset_parameters(self):\n        stdv=math.sqrt(6./((self.in_channels*(self.kernel_size[0]**2))+(self.out_channels*(self.kernel_size[0]**2))))\n        self.weight.data.uniform_(-stdv,stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv,stdv)\n        self.bias1.data.uniform_(-stdv,stdv)\n    \n    \n    def forward(self, input_):\n\n        hout = ((input_.shape[2]+2*self.padding[0]-self.calculated_kernel_size)//self.stride[0])+1\n        wout = ((input_.shape[3]+2*self.padding[1]-self.calculated_kernel_size)//self.stride[1])+1\n               \n        weight_kernel = F.unfold(input_,kernel_size=self.kernel_size,dilation=self.dilation,stride=self.stride).to(self.device)\n        fuzzy_kernel = F.unfold(input_,kernel_size=self.calculated_kernel_size,dilation=1,stride=self.stride).to(self.device)\n        \n        convolvedOutput = (fuzzy_kernel.transpose(1,2).matmul((((self.fuz.permute(1,2,3,0))*self.bias1).permute(3,0,1,2)).flatten(1).transpose(0,1))).transpose(1,2)+(weight_kernel.transpose(1,2).matmul((self.weight).flatten(1).transpose(0,1))).transpose(1,2)\n        convolutionReconstruction=convolvedOutput.view(input_.shape[0],self.out_channels,hout,wout)\n        return convolutionReconstruction","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TOFU(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = Conv2d( in_channels, 16, kernel_size=3, dilation=1, padding=1, padding_mode='replicate' )\n        self.conv2 = Conv2d( 16,32, kernel_size=3, dilation=3, padding=3, padding_mode='replicate')\n        self.conv3 = Conv2d(16+32,48, kernel_size=3, dilation=5, padding=5, padding_mode='replicate')\n        \n        self.compress = Conv2d(16+32+48,out_channels, kernel_size=1)\n        self.bn = nn.BatchNorm2d(out_channels)\n        \n    def forward(self, x):\n        x1 = F.relu(self.conv1(x))\n        x2 = F.relu(self.conv2(x1))\n        x3 = F.relu(self.conv3(torch.cat([x1,x2],1)))\n        x4 = F.relu(self.compress(torch.cat([x1,x2,x3],1)))\n        x4 = self.bn(x4)\n        return x4\n        \nclass MOFU(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.fconv1 = CustomConv(in_channels,16,kernel_size=3, stride=2,dilation=2, mu=0.1)\n        self.fconv2 = CustomConv(16,32,kernel_size=3, stride=3,dilation=3, mu=0.3)\n        self.fconv3 = CustomConv(32,out_channels,kernel_size=3, stride=5,dilation=5, mu=0.5)\n        self.bn = nn.BatchNorm2d(out_channels)\n        \n    def forward(self, x):\n        x = F.relu(self.fconv1(x))\n        x = F.relu(self.fconv2(x))\n        x = F.relu(self.fconv3(x))\n        x = self.bn(x)\n        return x\n        \n\nclass Net(nn.Module): \n    def __init__(self, num_classes): \n        super().__init__()\n        self.tofu = TOFU(3,32)\n        self.mofu = MOFU(32,32)\n        self.fc = nn.Linear(32*15*15, num_classes) \n        self.gradients = None\n        \n    def forward(self, x):\n        x = self.tofu(x)\n        x = x[:,torch.randperm(x.size()[1])]\n        x = self.mofu(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\nnet = Net(num_classes)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision import models\nmodel = net.to(device)\npytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(\"Total trainable params:\", pytorch_total_params)\npytorch_total_params = sum(p.numel() for p in model.parameters())\nprint(\"All params:\", pytorch_total_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss and optimizer\ncriterion = FocalLoss()\noptimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, eta_min=0.0002)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"/input/covid-cxr-3-layers/final_state.dct\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval(model):\n    model.eval()  \n    with torch.no_grad():\n        correct = 0\n        total = 0\n        loss = 0\n        \n        for images, labels in loader_valid:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            \n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n            loss += criterion(outputs, labels)\n\n        val_loss = loss / len(loader_valid)\n        acc = 100 * correct / total\n\n    return(acc, val_loss)\n\ndef test(model):\n    model.eval()  \n    with torch.no_grad():\n        correct = 0\n        total = 0\n        y_true = []\n        y_pred = []\n        for images, labels in loader_test:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            y_true.append(labels.cpu().detach())\n            y_pred.append(predicted.cpu().detach())\n            \n        y_true = torch.cat(y_true).numpy()\n        y_pred = torch.cat(y_pred).numpy()\n        print(\"f1:\\t\",f1_score(y_true, y_pred, average='macro'))\n        print(\"prec:\\t\",precision_score(y_true, y_pred, average='macro'))\n        print(\"recall:\\t\",recall_score(y_true, y_pred, average='macro'))\n    return(100 * correct / total)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\ntotal_step = len(loader_train)\ntrain_howlong = HowLong()\n\nfor epoch in range(num_epochs):\n    train_losses = []\n    print(\"Epoch \", epoch+1,\" started...\")\n    epoch_howlong = HowLong()\n    model.train()\n    \n    for i, (images, labels) in enumerate(loader_train):\n        images = images.to(device)\n        labels = labels.to(device)\n                \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step(epoch)\n        train_losses.append(loss.item())\n\n        if (i+1) % 100 == 0:\n            print ('  Epoch [{:2d}/{:2d}] \\t Step [{:3d}/{:3d}] \\t Train Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))                \n            \n    acc, val_loss = eval(model)\n    print ('  Val Loss: {:.4f} \\t Val Acc: {:.2f}'.format(val_loss.item(), acc))    \n    \n    hist.add_metric(train_loss=mean(train_losses), valid_loss=val_loss.item(), valid_acc=acc)\n    hist.save_checkpoint(model.state_dict(), val_loss.item())\n    \n    epoch_howlong.since_last()\n    \n\ntrain_howlong.since_start()\nhist.save_checkpoint(model.state_dict(), val_loss.item(), name=\"final_state\", unconditional=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Evaluating model...\")\n\nacc = test(model)\nprint(acc)\n\ntotal_howlong.since_start()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fvcore.nn import FlopCountAnalysis, flop_count_table\nimages, labels =  next(iter(loader_test))\nimage = images[0].unsqueeze(0)\nimage = image.to(device)\nflops = FlopCountAnalysis(model, image)\nprint(flops.total())\nprint(flop_count_table(flops))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(hist.get_metrics())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Run the following cells to obtain gradcam image****","metadata":{}},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n\nclass Net(nn.Module): \n    def __init__(self, num_classes): \n        super().__init__()\n        self.tofu = TOFU(3,32)\n        self.mofu = MOFU(32,32)\n        self.fc = nn.Linear(32*15*15, num_classes) \n        self.gradients = None\n    def activations_hook(self, grad):\n        self.gradients = grad\n    def get_activations_gradient(self):\n        return self.gradients\n    \n    # method for the activation exctraction\n    def get_activations(self, x):\n        return self.features_conv(x)\n    def forward(self, x):\n        x = self.tofu(x)\n        x = self.mofu.fconv1(x)\n        x = self.mofu.fconv2(x)\n        x = self.mofu.fconv3(x)\n        h = x.register_hook(self.activations_hook)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\nnet = Net(num_classes)\nmodel=net.to(device)\ncriterion = FocalLoss()\noptimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#to use latest weights after model is trained, replace the following uncommented code by the commented code\nmodel.load_state_dict(torch.load(\"../input/covid-cxr-3-layers/final_state.dct\"))\n#model.load_state_dict(torch.load(\"./final_state.dct\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating gradcam image\nimg_path=\"../input/covidx-cxr2/train/000001-1.jpg\"\nimage = cv2.imread(img_path)\nimage = cv2.resize(image, (DIM,DIM))\ntrans = transforms.Compose([transforms.ToPILImage(),                    \n                                  transforms.Pad(64, padding_mode='reflect'),\n                                  transforms.Resize(DIM, interpolation = 2),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\nimg = trans(image).to(device)\npred = model(img.unsqueeze(0))#.argmax(dim=1)\npred.shape\npred[:,torch.argmax(pred)].backward()\ngradients = net.get_activations_gradient()\ngradients.shape\npooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\nactivations = net.tofu(img.unsqueeze(0))\nactivations = net.mofu.fconv1(activations)\nactivations = net.mofu.fconv2(activations)\nactivations = net.mofu.fconv3(activations).detach()\nactivations.shape\nfor i in range(32):\n    activations[:, i, :, :] *= pooled_gradients[i]\nheatmap = torch.mean(activations, dim=1).squeeze()\nheatmap = np.maximum(heatmap.cpu(), 0)\nheatmap /= torch.max(heatmap)\nplt.matshow(heatmap.squeeze())\nimg=img.permute(1,2,0)\nheatmap = cv2.resize(heatmap.numpy(), (img.shape[0], img.shape[1]))\nheatmap = np.uint8(255 * heatmap)\nheatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\nsuperimposed_img = heatmap * 0.1 + img.cpu().numpy()\ncv2.imwrite('./map.jpg', superimposed_img)\nplt.imshow( heatmap * 0.002 + img.cpu().numpy()  )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save gradcam and original image\nx=heatmap * 0.002 + img.cpu().numpy()\ny=np.transpose(x, axes=[2,0,1])\ny=torch.tensor(y)\nfrom torchvision.utils import save_image\nsave_image(y, 'gradcam1.png')\nimg=img.permute(2, 0, 1)\nsave_image(img, 'original1.png')","metadata":{},"execution_count":null,"outputs":[]}]}